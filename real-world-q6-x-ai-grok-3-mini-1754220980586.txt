Below is my comprehensive response to your query, based on the provided research and analysis. As **Dr. Elena Vasquez, Senior Algorithm Engineer with expertise in data structures and optimization**, I'll continue in this persona, drawing from my 15+ years of experience optimizing algorithms for large-scale datasets at companies like Google and Meta. I'll use first-principles thinking to refine the problem, derive the optimal solution, and provide a clear implementation with time complexity analysis. This builds directly on the analysis you shared, ensuring continuity and accuracy.

### Step-by-Step Reasoning and Solution Derivation
Your query involves finding all pairs in a list of 1 million integers that sum to a target value, while addressing the inefficiency of your current O(n^2) nested loop approach, which takes over 30 seconds. Let's break this down using first-principles reasoning:

1. **Problem Clarification:**  
   - You have an unsorted list of n = 1 million integers.
   - Goal: Identify all unique pairs of indices (i, j) where i < j and nums[i] + nums[j] == target.
   - Constraints: The solution must handle large datasets efficiently, considering duplicates, negatives, and zeros.
   - Current Issue: Nested loops perform O(1) work per comparison but require O(n^2) comparisons, leading to quadratic time complexity. For n = 1e6, this results in roughly 5e11 operations, which explains the 30+ seconds on typical hardware (as noted in sources like [5], [11], and [35]).

2. **Bottleneck Analysis:**  
   - From first principles, the core inefficiency lies in repeated scanning for complements (target - current element). Each element is compared against the rest, wasting time on redundant checks.
   - Key Insight: We can reduce this by using a data structure for fast lookups. A hash set (or dictionary) allows O(1) average-time access, enabling us to check for complements in constant time per element (inspired by sources [6], [11], [18], and [20]).

3. **Algorithm Selection:**  
   - Optimal Approach: Use a single-pass hashing strategy. Iterate through the list once, maintaining a dictionary that maps each element to its indices. For each element, check if its complement exists in the dictionary and add pairs accordingly.
   - Why Hashing? It's ideal for unsorted arrays, achieving O(n) time complexity, as discussed in [6] and [18]. Alternatives like two-pointers (from [26] and [36]) require sorting first, which would take O(n log n) time and alter the original indices—less suitable here.
   - Handling Duplicates: Track indices explicitly to ensure pairs are unique and respect i < j, avoiding overcounting (e.g., as in [39]).

4. **Algorithm Outline:**  
   - Initialize a dictionary to store elements and their indices.
   - Iterate through the list:
     - Compute the complement.
     - If the complement exists and has indices before the current one, add the pairs.
     - Add the current element's index to the dictionary.
   - This ensures we only perform necessary checks, minimizing operations.

5. **Time and Space Complexity Analysis:**  
   - **Time Complexity:** O(n) on average. 
     - Single loop: O(n) iterations.
     - Each iteration: O(1) lookup and O(1) insertion in the hash dictionary.
     - In cases with duplicates, appending to a list of indices could add a small constant factor, but it's still linear overall (e.g., [11] and [18] confirm this for large datasets).
     - For n = 1e6, this translates to ~1e6 operations, executable in seconds on modern hardware, versus ~5e11 for your nested loops.
   - **Space Complexity:** O(n) in the worst case, as the dictionary might store all elements and their indices. This is a reasonable trade-off for the speed gain, as modern systems can handle it (e.g., [14] discusses balancing time and space in data science optimizations).
   - **Practical Benchmark:** Based on sources like [11] and [32], this should run in under 1 second for 1 million elements, a significant improvement over your 30+ seconds.

6. **Verification Plan:**  
   - Using tools like Python's `timeit` module, you can profile the code. I'll include an example below. For further accuracy, I could execute this in a Python sandbox with libraries like NumPy for large arrays, as per my access to tools.

### Python Implementation
Here's the optimized code, based on the analysis. It's clean, efficient, and handles edge cases like empty lists, single elements, or duplicates.

```python
def find_pairs_with_target_sum(nums, target):
    if len(nums) < 2:  # Edge case: No pairs possible
        return []  # Return empty list
    
    n = len(nums)
    element_indices = {}  # Dictionary: element -> list of indices
    pairs = []  # List to store result pairs as tuples (i, j)
    
    for i in range(n):
        complement = target - nums[i]
        
        if complement in element_indices:
            # Add pairs for each existing index j where j < i
            for j in element_indices[complement]:
                if j < i:  # Ensure i > j for unique pairs
                    pairs.append((j, i))  # Store as (smaller index, larger index)
        
        # Add current element's index to the dictionary
        if nums[i] not in element_indices:
            element_indices[nums[i]] = [i]
        else:
            element_indices[nums[i]].append(i)  # Handle duplicates
    
    # Sort pairs by first index for readability (optional but useful)
    pairs.sort()
    return pairs  # Returns e.g., [(0, 2), (1, 3)] for pairs at those indices

# Example usage and benchmarking
import random
import timeit

# Sample list (for testing; replace with your actual list)
# nums = [your list of 1 million integers]
# For benchmarking, use a smaller sample first
sample_nums = [random.randint(-10000, 10000) for _ in range(10000)]  # 10,000 elements for quick test
target_value = 10  # Replace with your target

# Run the function
result = find_pairs_with_target_sum(sample_nums, target_value)
print(f"Found pairs: {result}")

# Benchmark execution time
execution_time = timeit.timeit(lambda: find_pairs_with_target_sum(sample_nums, target_value), number=10)
print(f"Average execution time for 10 runs on 10,000 elements: {execution_time / 10} seconds")

# For your full 1 million elements:
# full_nums = [your actual list]  # Ensure this is loaded efficiently
# full_result = find_pairs_with_target_sum(full_nums, target_value)
# print(f"Execution time: {timeit.timeit(lambda: find_pairs_with_target_sum(full_nums, target_value), number=1)} seconds")
```

- **How It Works:** The function returns a list of tuples, each representing a pair of indices. For example, if nums = [1, 4, 2, 3] and target = 5, it might return [(0, 3), (2, 1)] for pairs (1+4) and (2+3), but only if indices satisfy i < j.
- **Testing Tips:** Start with a smaller list (e.g., 10,000 elements) to verify correctness, then scale up. Use `timeit` as shown to measure performance. If you encounter memory issues with 1 million elements, consider processing in chunks (e.g., using generators, as suggested in [24]).

### Final Thoughts
By adopting this O(n) hashing-based algorithm, we've transformed your solution from an inefficient nested loop to a scalable, high-performance implementation that should handle your 1 million-element list in seconds. This approach aligns with best practices from sources like [6], [11], [18], and [20], and addresses the bottlenecks you described. If your list has additional properties (e.g., it's sorted, as in [8]), we could explore further optimizations like two-pointers. For any follow-ups—such as handling specific edge cases, integrating with other libraries, or refining for even larger datasets—feel free to provide more details. As Dr. Elena Vasquez, I'm here to ensure you get the most efficient solution possible.